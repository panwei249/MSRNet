import os
import numpy as np
import scipy.io as sio
import numpy as np
import os
from scipy.io import loadmat
import os
import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import torch


def load_data_de(path, subject):
    label_seed4 = [[1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3],
                   [2, 1, 3, 0, 0, 2, 0, 2, 3, 3, 2, 3, 2, 0, 1, 1, 2, 1, 0, 3, 0, 1, 3, 1],
                   [1, 2, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 3, 3, 0, 2, 3, 0, 0, 2, 0, 1, 0]]
    """åŠ è½½SEED-IVæ•°æ®é›†çš„ä¾èµ–å®éªŒæ•°æ®

    Args:
        path: æ•°æ®æ–‡ä»¶è·¯å¾„
        subject: è¢«è¯•æ–‡ä»¶åï¼ˆä¾‹å¦‚ï¼š2_20150920.matï¼‰

    Returns:
        åŒ…å«è®­ç»ƒå’Œæµ‹è¯•æ•°æ®çš„å­—å…¸
    """
    mat_path = os.path.join(path, subject)
    data = loadmat(mat_path)

    samples = []
    labels = []

    # è°ƒè¯•ä¿¡æ¯
    print(f"Loading data from: {mat_path}")

    # SEED-IVæ•°æ®é›†ä¸­æ¯ä¸ªtrialçš„å·®åˆ†ç†µç‰¹å¾
    for i in range(1, 25):  # 24ä¸ªtrial
        key = f'de_LDS{i}'
        if key in data:
            print("111111111111111")
            print(key)
            # è·å–å½“å‰trialçš„æ•°æ®
            trial_data = data[key]  # shape: (62, time_steps, 5)

            # # å¯¹æ—¶é—´ç»´åº¦è¿›è¡Œå¹³å‡
            # if len(trial_data.shape) == 3:
            #     trial_data = np.mean(trial_data, axis=1)  # shape: (62, 5)
            #
            # # ç¡®ä¿æ•°æ®ç±»å‹å’Œç»´åº¦æ­£ç¡®
            # trial_data = trial_data.astype(np.float32)
            #
            # if trial_data.shape != (62, 5):
            #     print(f"Warning: Invalid shape for {key} after processing: {trial_data.shape}")
            #     continue
            #
            # samples.append(trial_data)
            #
            # # SEED-IVçš„æƒ…æ„Ÿæ ‡ç­¾: 0-å¹³é™, 1-æ‚²ä¼¤, 2-ææƒ§, 3-é«˜å…´
            # labels=label_seed4[0]
            trial_data = trial_data.astype(np.float32)

            # éå†æ—¶é—´ç»´åº¦çš„æ¯ä¸ªåˆ‡ç‰‡
            if len(trial_data.shape) == 3:
                time_steps = trial_data.shape[1]
                for t in range(time_steps):
                    time_slice = trial_data[:, t, :]  # å•ä¸ªæ—¶é—´ç‰‡æ®µ, shape: (62, 5)
                    samples.append(time_slice)
                    labels.append(label_seed4[0][i - 1])  # å¯¹åº”trialçš„æ ‡ç­¾

    if not samples:
        raise ValueError(f"No valid samples found in {subject}")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„å‰æ£€æŸ¥ç»´åº¦
    print(f"Number of samples collected: {len(samples)}")
    print(f"Shape of first sample: {samples[0].shape}")

    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    try:
        samples = np.stack(samples)  # shape: (n_trials, 62, 5)
        labels = np.array(labels)

        # æ·»åŠ æ—¶é—´ç»´åº¦
        # samples = np.expand_dims(samples, axis=2)  # shape: (n_trials, 62, 1, 5)

        print(f"Final samples shape: {samples.shape}")
        print(f"Final labels shape: {labels.shape}")
        print(f"Label distribution: {np.bincount(labels)}")

        # å¯é€‰ï¼šå¯¹æ¯ä¸ªtrialçš„ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–

    except ValueError as e:
        print("Error stacking samples. Sample shapes:")
        for i, s in enumerate(samples):
            print(f"Sample {i} shape: {s.shape}")
        raise

    # éšæœºæ‰“ä¹±æ•°æ®
    indices = np.random.permutation(len(samples))
    samples = samples[indices]
    labels = labels[indices]

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ80%è®­ç»ƒï¼Œ20%æµ‹è¯•ï¼‰
    n_train = int(0.6 * len(samples))

    return {
        "x_tr": samples[:n_train],  # shape: (19, 62, 5)
        "y_tr": labels[:n_train],  # shape: (19,)
        "x_ts": samples[n_train:],  # shape: (5, 62, 5)
        "y_ts": labels[n_train:]  # shape: (5,)
    }


def load_data_de1(path, subject):
    """
    å°†æ¯ä¸ª trial çš„æ—¶é—´ç»´åº¦æ‹†åˆ†æˆå¤šä¸ªæ ·æœ¬ã€‚
    åŒæ—¶å°†åŸå§‹æ ‡ç­¾ -1, 0, 1 æ˜ å°„åˆ° 0, 1, 2ã€‚
    """

    # 1. è¯»å–ä¸»ä½“æ•°æ®
    mat_path = os.path.join(path, subject)
    data = loadmat(mat_path)  # ä¾‹å¦‚ shape: (62, time_steps, 5)

    # 2. è¯»å–æ ‡ç­¾æ–‡ä»¶
    mat_path1 = os.path.join(path, "label.mat")
    label_dict = loadmat(mat_path1)
    if 'label' not in label_dict.keys():
        raise ValueError("label.mat æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'label' é”®ã€‚")

    # å½¢å¦‚ (15,), é‡Œå¤´å¯èƒ½åŒ…å« -1, 0, 1
    label_data = label_dict['label'].squeeze()
    print("label_data shape:", label_data.shape)
    print("label_data:", label_data)

    samples = []
    labels = []

    print(f"Loading data from: {mat_path}")

    # å‡è®¾æœ‰ 15 ä¸ª trialï¼ˆè‹¥æœ‰ 24 ä¸ªåˆ™æŠŠ range æ”¹ä¸º range(1,25)ï¼‰
    for i in range(1, 16):
        key = f'de_LDS{i}'
        if key not in data:
            print(f"Warning: {key} not found in {subject}, skip.")
            continue

        # trial_data: (62, time_steps, 5)
        trial_data = data[key].astype(np.float32)
        print(trial_data.shape)

        time_steps = trial_data.shape[1]

        for t in range(time_steps):
            time_slice = trial_data[:, t, :]
            samples.append(time_slice)

            # --- è¿™é‡Œå¯¹æ ‡ç­¾è¿›è¡Œåç§»å¤„ç† ---
            # å¦‚æœåŸå§‹æ ‡ç­¾ä»…æœ‰ -1,0,1ï¼Œåˆ™è®© -1->0, 0->1, 1->2
            raw_label = label_data[i - 1]  # åŸå§‹æ ‡ç­¾
            mapped_label = raw_label + 1  # åç§» 1
            # å¦‚æœåŸæœ¬æ˜¯ -1ï¼Œåˆ™ mapped_label=0ï¼›åŸæœ¬æ˜¯0->1ï¼›åŸæœ¬æ˜¯1->2
            labels.append(mapped_label)

    if not samples:
        raise ValueError(f"No valid samples found in {subject}")

    print(f"Number of samples collected: {len(samples)}")
    print(f"Shape of first sample: {samples[0].shape}")

    # stack æˆ (n_total, 62, 5)
    samples = np.stack(samples)
    labels = np.array(labels, dtype=np.int32)

    # æ‰©å±•ç»´åº¦ -> (n_total, 62, 1, 5)
    samples = np.expand_dims(samples, axis=2)

    print(f"Final samples shape: {samples.shape}")
    print(f"Final labels shape: {labels.shape}")

    # å¯é€‰çš„å½’ä¸€åŒ–

    # æ‰“ä¹±
    indices = np.random.permutation(len(samples))
    samples = samples[indices]
    labels = labels[indices]

    # åˆ’åˆ†è®­ç»ƒé›† (80%) å’Œ æµ‹è¯•é›† (20%)
    n_train = int(0.8 * len(samples))
    return {
        "x_tr": samples[:n_train],
        "y_tr": labels[:n_train],
        "x_ts": samples[n_train:],
        "y_ts": labels[n_train:]
    }


def load_data_de2(path, subject, session):
    pass


def load_data_de3(path, subject):
    pass


def load_data_vmd(path, subject):
    mat_path = os.path.join(path, subject)
    data = np.load(mat_path, allow_pickle=True)
    data = data.item()
    # ä¾‹å¦‚ shape: (62, time_steps, 5)

    # 2. è¯»å–æ ‡ç­¾æ–‡ä»¶
    mat_path1 = os.path.join(path, "label.mat")
    label_dict = loadmat(mat_path1)
    if 'label' not in label_dict.keys():
        raise ValueError("label.mat æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'label' é”®ã€‚")

    # å½¢å¦‚ (15,), é‡Œå¤´å¯èƒ½åŒ…å« -1, 0, 1
    label_data = label_dict['label'].squeeze()
    print("label_data shape:", label_data.shape)
    print("label_data:", label_data)

    label_data = label_dict['label'].squeeze()  # shape: (15,)
    print(label_data)
    for i in range(0, 15):
        label_data[i] = label_data[i] + 1
    print(label_data)
    if label_data.shape[0] != 15:
        raise ValueError(f"æœŸæœ› label å¤§å°ä¸º15ï¼Œå®é™…ä¸º {label_data.shape[0]}ã€‚")

    print(f"åŠ è½½æ–‡ä»¶: {mat_path}")
    print(f"åŠ è½½æ ‡ç­¾: {label_data}, label shape = {label_data.shape}")

    # 3. æ”¶é›†æ ·æœ¬ä¸æ ‡ç­¾
    samples_list = []
    labels_list = []

    # ç­›é€‰å‡ºé '__xxx__' å¼€å¤´çš„é”®ï¼Œé€šå¸¸æ˜¯æœ‰æ•ˆä¼šè¯
    keys = [k for k in data.keys() if not k.startswith('__')]
    if len(keys) != 15:
        print(f"è­¦å‘Š: {subject} ä¸­å®é™…ä¼šè¯æ•°é‡ä¸º {len(keys)}ï¼Œè€Œé 15ï¼Œè¯·æ ¸å¯¹æ•°æ®æ–‡ä»¶ã€‚")

    # å‡è®¾ keys çš„é¡ºåºä¸ label_data ä¸€ä¸€å¯¹åº”
    # è‹¥é¡ºåºä¸ä¸€è‡´ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µæ’åºæˆ–åŒ¹é…
    for i, key in enumerate(keys):
        session_data = data[key]  # shape: (time, 62, 5)
        if not isinstance(session_data, np.ndarray):
            print(f"è·³è¿‡ {key}, æ•°æ®ç±»å‹ä¸æ˜¯ ndarray")
            continue

        # ç¡®ä¿æ˜¯ float32
        session_data = session_data.astype(np.float32)

        # time, 62, 5
        time_steps = session_data.shape[0]
        if session_data.shape[1] != 62 or session_data.shape[2] != 5:
            print(f"è­¦å‘Š: {key} çš„ç»´åº¦ä¸æ˜¯ (time, 62, 5)ï¼Œå®é™…ä¸º {session_data.shape}")

        # å°†æ¯ä¸ªæ—¶é—´æ­¥è§†ä¸ºä¸€ä¸ªæ ·æœ¬ (time_steps, 62, 5)
        # å¦‚æœæ‚¨æƒ³ä¿ç•™æ—¶åºä¿¡æ¯ï¼Œå¯ä¸è¦ reshape
        # ä¹Ÿå¯æ ¹æ®éœ€è¦æ·»åŠ é¢å¤–ç»´åº¦
        # è¿™é‡Œç›´æ¥ä½¿ç”¨ (time_steps, 62, 5)
        # å¯¹æ¯ä¸ªæ—¶é—´æ­¥èµ‹åŒæ ·çš„æ ‡ç­¾
        label_val = label_data[i]

        samples_list.append(session_data)  # (time_steps, 62, 5)
        labels_list.append(np.full((time_steps,), label_val, dtype=np.int32))

    if not samples_list:
        raise ValueError(f"{subject} ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ä¼šè¯æ•°æ®æˆ–é”®ã€‚")

    # 4. åˆå¹¶æ‰€æœ‰ä¼šè¯
    #   - X shape: (sum_time, 62, 5)
    #   - Y shape: (sum_time,)
    X = np.concatenate(samples_list, axis=0)
    Y = np.concatenate(labels_list, axis=0)

    print(f"åˆå¹¶å X shape = {X.shape}, Y shape = {Y.shape}")

    # 5. è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†
    test_size = 0.4
    random_state = 42
    x_tr, x_ts, y_tr, y_ts = train_test_split(
        X, Y, test_size=test_size, random_state=random_state, shuffle=True
    )
    print(f"è®­ç»ƒé›†å¤§å°: {x_tr.shape[0]}, æµ‹è¯•é›†å¤§å°: {x_ts.shape[0]}")
    # x_tr = np.expand_dims(x_tr, axis=2)
    # x_ts = np.expand_dims(x_ts, axis=2)
    print(x_tr.shape)
    print(y_tr.shape)
    print(x_ts.shape)
    print(y_ts.shape)

    # 6. è¿”å›ç»“æœ
    return {
        "x_tr": x_tr,  # shape: (N_train, 62, 5)
        "y_tr": y_tr,
        "x_ts": x_ts,  # shape: (N_test, 62, 5)
        "y_ts": y_ts
    }


def load_data_vmd1(path, subject):
    # 1. åŠ è½½æ•°æ®æ–‡ä»¶
    mat_path = os.path.join(path, subject)
    data = np.load(mat_path, allow_pickle=True)
    data = data.item()
    # å‡è®¾æ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªä¼šè¯ï¼Œæ•°æ®å½¢çŠ¶ä¸º (time_steps, 62, 5)

    # 2. å®šä¹‰æ–°çš„æ ‡ç­¾ label_seed4ï¼Œå…± 24 ä¸ªæ ‡ç­¾
    label_seed4 = [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3]
    label_seed4 = np.array(label_seed4, dtype=np.int32)

    # 3. æ”¶é›†æ ·æœ¬ä¸æ ‡ç­¾
    samples_list = []
    labels_list = []

    # ç­›é€‰å‡ºé '__xxx__' å¼€å¤´çš„é”®ï¼ˆå…± 24 ä¸ªé”®ï¼‰
    keys = [k for k in data.keys() if not k.startswith('__')]
    if len(keys) != 24:
        print(f"è­¦å‘Š: æ•°æ®æ–‡ä»¶ä¸­æœ‰æ•ˆä¼šè¯æ•°é‡ä¸º {len(keys)}ï¼Œè€Œé 24ï¼Œè¯·æ ¸å¯¹æ•°æ®æ–‡ä»¶ã€‚")

    # ä¸ºç¡®ä¿é¡ºåºä¸€è‡´ï¼Œå¯ä»¥å¯¹é”®è¿›è¡Œæ’åºï¼ˆå‰ææ˜¯é”®åç§°å¯æ’åºï¼‰
    def extract_numeric_session(key):
        match = re.search(r'\d+', key)  # æå–æ•°å­—éƒ¨åˆ†
        return int(match.group()) if match else float('inf')

    keys = sorted(data.keys(), key=extract_numeric_session)

    # éå†æ¯ä¸ªä¼šè¯æ•°æ®ï¼Œå¹¶ä¸ºæ¯ä¸ªä¼šè¯çš„æ‰€æœ‰æ—¶é—´æ­¥èµ‹äºˆå¯¹åº”çš„æ ‡ç­¾
    for i, key in enumerate(keys):
        print(i)
        print(key)
        session_data = data[key]  # å½¢çŠ¶: (time_steps, 62, 5)
        if not isinstance(session_data, np.ndarray):
            print(f"è·³è¿‡ {key}ï¼Œæ•°æ®ç±»å‹ä¸æ˜¯ ndarray")
            continue

        # ç¡®ä¿æ•°æ®ä¸º float32 ç±»å‹
        session_data = session_data.astype(np.float32)
        time_steps = session_data.shape[0]
        if session_data.shape[1] != 62 or session_data.shape[2] != 5:
            print(f"è­¦å‘Š: {key} çš„æ•°æ®å½¢çŠ¶ä¸º {session_data.shape}ï¼Œè€Œé (time_steps, 62, 5)")

        # å½“å‰ä¼šè¯å¯¹åº”çš„æ ‡ç­¾
        label_val = label_seed4[i]
        # ä¸ºå½“å‰ä¼šè¯çš„æ‰€æœ‰æ—¶é—´æ­¥ç”Ÿæˆç›¸åŒçš„æ ‡ç­¾æ•°ç»„
        session_labels = np.full((time_steps,), label_val, dtype=np.int32)

        samples_list.append(session_data)
        labels_list.append(session_labels)

    if not samples_list:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ä¼šè¯æ•°æ®ã€‚")

    # 4. åˆå¹¶æ‰€æœ‰ä¼šè¯æ•°æ®
    #    X shape: (æ€»æ—¶é—´æ­¥, 62, 5)
    #    Y shape: (æ€»æ—¶é—´æ­¥,)
    X = np.concatenate(samples_list, axis=0)
    Y = np.concatenate(labels_list, axis=0)

    print(f"åˆå¹¶å X shape = {X.shape}, Y shape = {Y.shape}")

    # 5. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    test_size = 0.4
    random_state = 42
    x_tr, x_ts, y_tr, y_ts = train_test_split(
        X, Y, test_size=test_size, random_state=random_state, shuffle=True
    )
    print(f"è®­ç»ƒé›†å¤§å°: {x_tr.shape[0]}, æµ‹è¯•é›†å¤§å°: {x_ts.shape[0]}")
    print("x_tr shape:", x_tr.shape)
    print("y_tr shape:", y_tr.shape)
    print("x_ts shape:", x_ts.shape)
    print("y_ts shape:", y_ts.shape)

    # 6. è¿”å›ç»“æœ
    return {
        "x_tr": x_tr,  # shape: (N_train, 62, 5)
        "y_tr": y_tr,
        "x_ts": x_ts,  # shape: (N_test, 62, 5)
        "y_ts": y_ts
    }


import os
import numpy as np
from scipy.io import loadmat


def load_data_vmd_leave_one_subject_out(path, test_subject, random_state=42):
    """
    ç•™ä¸€ä¸ªä¸ªä½“ä½œä¸ºæµ‹è¯•é›† (Leave-One-Subject-Out Cross Validation, LOSO)

    å‚æ•°ï¼š
    - path: æ•°æ®æ–‡ä»¶æ‰€åœ¨ç›®å½•
    - test_subject: ä½œä¸ºæµ‹è¯•é›†çš„å—è¯•è€…æ–‡ä»¶å (åº”ä¸º .npy æ–‡ä»¶)
    - random_state: éšæœºç§å­ï¼Œä¿è¯å¯å¤ç°

    è¿”å›ï¼š
    - dict, åŒ…å«è®­ç»ƒ/æµ‹è¯•æ•°æ®ï¼š
        {
            "x_tr": np.ndarray, # è®­ç»ƒæ•°æ® (N_train, 62, 5)
            "y_tr": np.ndarray, # è®­ç»ƒæ ‡ç­¾ (N_train,)
            "x_ts": np.ndarray, # æµ‹è¯•æ•°æ® (N_test, 62, 5)
            "y_ts": np.ndarray  # æµ‹è¯•æ ‡ç­¾ (N_test,)
        }
    """
    # è·å–æ‰€æœ‰å—è¯•è€…æ–‡ä»¶ï¼ˆæ’é™¤ label.matï¼‰
    subjects = [f for f in os.listdir(path) if f.endswith('.npy') and f != "label.mat"]
    if len(subjects) < 2:
        raise ValueError("æ•°æ®é›†å—è¯•è€…æ•°é‡è¿‡å°‘ï¼Œè‡³å°‘éœ€è¦ 2 ä¸ªå—è¯•è€…æ‰èƒ½è¿›è¡Œ LOSO è®­ç»ƒï¼")

    # ç¡®ä¿æµ‹è¯•å—è¯•è€…å­˜åœ¨
    if test_subject not in subjects:
        raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•å—è¯•è€… {test_subject} ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åã€‚")

    # è¯»å–æ ‡ç­¾ä¿¡æ¯
    label_path = os.path.join(path, "label.mat")
    label_dict = loadmat(label_path)
    if 'label' not in label_dict:
        raise ValueError("label.mat æ–‡ä»¶ä¸­æœªæ‰¾åˆ° 'label' é”®")

    label_data = label_dict['label'].squeeze()  # shape: (N_subjects, 15) æ¯ä¸ªå—è¯•è€…æœ‰ 15 ä¸ªä¼šè¯çš„æ ‡ç­¾
    print(label_data)
    label_data += 1
    print(label_data)
    # if label_data.shape[0] != len(subjects) or label_data.shape[1] != 15:
    #     raise ValueError(f"æ ‡ç­¾çŸ©é˜µ {label_data.shape} ä¸å—è¯•è€…æ•°é‡ {len(subjects)} ä¸åŒ¹é…ï¼")

    # è®­ç»ƒå—è¯•è€…åˆ—è¡¨ï¼ˆæ’é™¤æµ‹è¯•å—è¯•è€…ï¼‰
    train_subjects = [subj for subj in subjects if subj != test_subject]

    print(f"è®­ç»ƒå—è¯•è€…æ•°é‡: {len(train_subjects)}, æµ‹è¯•å—è¯•è€…: {test_subject}")

    # **ğŸ”¹ åŠ è½½å•ä¸ªå—è¯•è€…çš„æ•°æ®**
    def load_subject_data(subject_list, is_test=False):
        samples_list, labels_list = [], []
        for subj in subject_list:
            subj_path = os.path.join(path, subj)
            data = np.load(subj_path, allow_pickle=True).item()

            # è·å–å½“å‰å—è¯•è€…çš„ç´¢å¼•ï¼Œä»¥ä¾¿æ­£ç¡®åŒ¹é… `label_data`
            # subj_index = subjects.index(subj)
            # subj_labels = label_data[subj_index]  # shape: (15,)

            # ç¡®ä¿ä¼šè¯é¡ºåºåŒ¹é… `label_data`
            def extract_numeric_session(key):
                """ ä»ä¼šè¯ key ä¸­æå–æ•°å­—éƒ¨åˆ†ï¼Œä¾‹å¦‚ '13_20140527_xyl_eeg1_VMD_DE8' -> 13 """
                match = re.search(r'\d+', key)  # æå–æ•°å­—éƒ¨åˆ†
                return int(match.group()) if match else float('inf')

            session_keys = sorted(data.keys(), key=extract_numeric_session)
            if len(session_keys) != 15:
                raise ValueError(f"{subj} å—è¯•è€…çš„ä¼šè¯æ•° {len(session_keys)} ä¸ç­‰äº 15ï¼Œè¯·æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ï¼")

            for i, key in enumerate(session_keys):
                # print(i)
                # print(key)
                session_data = data[key]  # shape: (time_steps, 62, 5)
                time_steps = session_data.shape[0]

                # èµ‹äºˆæ­£ç¡®çš„ä¼šè¯æ ‡ç­¾
                label = label_data[i]  # å½“å‰ä¼šè¯å¯¹åº”çš„æƒ…ç»ªæ ‡ç­¾
                samples_list.append(session_data)  # (time_steps, 62, 5)
                labels_list.append(np.full((time_steps,), label, dtype=np.int32))

        # **ğŸ”¹ åˆå¹¶æ•°æ®**
        x_data = np.concatenate(samples_list, axis=0)  # (N_samples, 62, 5)
        y_data = np.concatenate(labels_list, axis=0)  # (N_samples,)

        return x_data, y_data

    # åŠ è½½è®­ç»ƒæ•°æ®
    x_tr, y_tr = load_subject_data(train_subjects)

    # åŠ è½½æµ‹è¯•æ•°æ®
    x_ts, y_ts = load_subject_data([test_subject], is_test=True)

    print(f"è®­ç»ƒé›†å¤§å°: {x_tr.shape[0]}, æµ‹è¯•é›†å¤§å°: {x_ts.shape[0]}")
    print(f"x_tr shape: {x_tr.shape}, y_tr shape: {y_tr.shape}")
    print(f"x_ts shape: {x_ts.shape}, y_ts shape: {y_ts.shape}")

    return {
        "x_tr": x_tr,  # shape: (N_train, 62, 5)
        "y_tr": y_tr,
        "x_ts": x_ts,  # shape: (N_test, 62, 5)
        "y_ts": y_ts
    }


# def load_data_inde(path, subject):
#     """
#     Independentå®éªŒæ•°æ®åŠ è½½ï¼š
#       - ä»¥ `subject` å¯¹åº”æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†
#       - ç›®å½•ä¸‹å…¶ä»–æ‰€æœ‰ .mat æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†
#     å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶éƒ½åŒ…å«ä¸ load_data_de ç±»ä¼¼çš„å·®åˆ†ç†µç‰¹å¾ de_LDS1~de_LDS24ï¼Œ
#     ä»¥åŠç”¨ label_seed4[0] æ¥ç»™ trial åšæ ‡ç­¾ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
#
#     å¦‚æœä½ çš„æ•°æ®æ ‡ç­¾/å¤„ç†é€»è¾‘ä¸åŒï¼Œè¯·åœ¨ parse_mat_file å‡½æ•°ä¸­è‡ªè¡Œä¿®æ”¹ã€‚
#     """
#
#     # å…ˆå®šä¹‰ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œç”¨æ¥è§£æå•ä¸ªè¢«è¯•çš„ .mat æ–‡ä»¶
#     def parse_mat_file(mat_path):
#         """
#         è¿™é‡Œçš„é€»è¾‘ä¸ä½  load_data_de ç±»ä¼¼ï¼Œ
#         è¯»å– 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„ shape: (62, time_steps, 5)
#         å¹¶å°†æ—¶é—´ç»´åº¦æ‹†æˆå¤šä¸ªæ ·æœ¬ï¼›ä»¥ label_seed4[0] ä½œä¸ºæ ‡ç­¾ã€‚
#         """
#         label_seed4 = [
#             [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3],
#             [2, 1, 3, 0, 0, 2, 0, 2, 3, 3, 2, 3, 2, 0, 1, 1, 2, 1, 0, 3, 0, 1, 3, 1],
#             [1, 2, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 3, 3, 0, 2, 3, 0, 0, 2, 0, 1, 0]
#         ]
#
#         data = loadmat(mat_path)
#         samples_list = []
#         labels_list = []
#
#         # è¿™é‡Œå†™æ­»äº†åªç”¨ label_seed4[0]ï¼Œå¦‚æœä½ æƒ³æ ¹æ®ä¸åŒè¢«è¯•/ä¼šè¯ç”¨å…¶ä»–è¡Œï¼Œè¯·æ”¹ä¸€ä¸‹ã€‚
#         for i in range(1, 25):  # å‡è®¾æ¯ä¸ªè¢«è¯•éƒ½æœ‰24ä¸ªtrial
#             key = f'de_LDS{i}'
#             if key in data:
#                 trial_data = data[key].astype(np.float32)  # (62, time_steps, 5)
#                 time_steps = trial_data.shape[1]
#
#                 # æ‹†åˆ†æ—¶é—´ç»´åº¦
#                 for t in range(time_steps):
#                     time_slice = trial_data[:, t, :]  # shape (62, 5)
#                     samples_list.append(time_slice)
#                     # è¿™é‡Œçš„æ ‡ç­¾å– label_seed4[0][i-1]ï¼Œä½ å¯ä»¥æ”¹æˆåˆ«çš„è¡Œæˆ–åˆ«çš„é€»è¾‘
#                     labels_list.append(label_seed4[0][i - 1])
#
#         if not samples_list:
#             raise ValueError(f"No valid trials found in {mat_path}")
#
#         # è½¬æˆ numpy
#         samples_arr = np.stack(samples_list)  # (n_samples, 62, 5)
#         labels_arr = np.array(labels_list, dtype=np.int32)
#
#
#         # å¦‚æœä½ éœ€è¦å¢åŠ ç»´åº¦å½¢çŠ¶ (n_samples, 62, 1, 5)
#         # samples_arr = np.expand_dims(samples_arr, axis=2)
#
#         return samples_arr, labels_arr
#
#     # åˆ†åˆ«æ”¶é›†â€œè®­ç»ƒç”¨â€çš„æ•°ç»„ å’Œ â€œæµ‹è¯•ç”¨â€çš„æ•°ç»„
#     train_samples, train_labels = [], []
#     test_samples, test_labels = [], []
#
#     # éå†ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
#     for filename in os.listdir(path):
#         # åªå¤„ç† .mat æ–‡ä»¶ï¼Œå¯æ ¹æ®å®é™…æƒ…å†µå†åšåˆ¤æ–­
#         if not filename.endswith('.mat'):
#             continue
#
#         # æ„é€ å®Œæ•´è·¯å¾„
#         mat_path = os.path.join(path, filename)
#
#         # å¦‚æœè¯¥æ–‡ä»¶å°±æ˜¯æˆ‘ä»¬æŒ‡å®šçš„ subject => åšæµ‹è¯•é›†
#         if filename == subject:
#             x_ts, y_ts = parse_mat_file(mat_path)
#             x_ts=standardize(x_ts)
#             test_samples.append(x_ts)
#             test_labels.append(y_ts)
#         else:
#             # å¦åˆ™å¹¶å…¥è®­ç»ƒé›†
#             x_tr, y_tr = parse_mat_file(mat_path)
#             x_tr = standardize(x_tr)
#             train_samples.append(x_tr)
#             train_labels.append(y_tr)
#
#     # å¦‚æœæµ‹è¯•è¢«è¯•æ²¡æ‰¾åˆ°ï¼Œå¯èƒ½ subject åå­—ä¸å¯¹ï¼Ÿ
#     if not test_samples:
#         raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•è¢«è¯•æ–‡ä»¶ {subject} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ .mat æ–‡ä»¶")
#
#     # åˆå¹¶æ‰€æœ‰è®­ç»ƒè¢«è¯•
#     if train_samples:
#         x_tr_merged = np.concatenate(train_samples, axis=0)
#         y_tr_merged = np.concatenate(train_labels, axis=0)
#     else:
#         raise ValueError("æ²¡æœ‰æ‰¾åˆ°é™¤æµ‹è¯•è¢«è¯•å¤–çš„ä»»ä½• .mat æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
#
#     # åˆå¹¶æµ‹è¯•è¢«è¯•ï¼ˆå¦‚æœ subject åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œä¸€èˆ¬å°±åªæœ‰ä¸€æ¬¡ appendï¼‰
#     x_ts_merged = np.concatenate(test_samples, axis=0)
#     y_ts_merged = np.concatenate(test_labels, axis=0)
#
#     print(f"ç‹¬ç«‹å®éªŒ: è®­ç»ƒæ ·æœ¬æ€»æ•° {x_tr_merged.shape[0]}, æµ‹è¯•æ ·æœ¬æ€»æ•° {x_ts_merged.shape[0]}")
#     print(f"è®­ç»ƒé›†å½¢çŠ¶: {x_tr_merged.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_ts_merged.shape}")
#     # ============ åœ¨è¿™é‡Œéšæœºæ‰“ä¹±è®­ç»ƒé›† =============
#     # train_perm = np.random.permutation(len(x_tr_merged))
#     # x_tr_merged = x_tr_merged[train_perm]
#     # y_tr_merged = y_tr_merged[train_perm]
#     #
#     # # å¦‚æœä¹Ÿæƒ³æ‰“ä¹±æµ‹è¯•é›†ï¼Œåˆ™å¯å†åŠ ä¸€æ®µ
#     # test_perm = np.random.permutation(len(x_ts_merged))
#     # x_ts_merged = x_ts_merged[test_perm]
#     # y_ts_merged = y_ts_merged[test_perm]
#     # x_tr_merged, x_ts_merged = standardize_data(x_tr_merged, x_ts_merged)
#
#
#     # ä¸åšäºŒæ¬¡åˆ‡åˆ†ï¼Œå› ä¸ºæˆ‘ä»¬å°±æ˜¯ (train vs. test) åœ¨è¢«è¯•å±‚é¢åˆ’åˆ†
#     return {
#         "x_tr": x_tr_merged,
#         "y_tr": y_tr_merged,
#         "x_ts": x_ts_merged,
#         "y_ts": y_ts_merged
#     }

import torch
import torch.nn as nn
import numpy as np
from scipy.io import loadmat

import torch
import torch.nn as nn
import torch.nn.functional as F


class SingleCoreResidualFusion(nn.Module):
    """
    ä»¥ f1 ä¸ºæ ¸å¿ƒç‰¹å¾ï¼Œf2 ä½œä¸ºè¾…åŠ©ç‰¹å¾ï¼š
      1) é€šè¿‡ Cross-Attention è®© f1 ä» f2 ä¸­è·å–ä¿¡æ¯
      2) å¾—åˆ°çš„æ³¨æ„åŠ›ç»“æœä¸åŸå§‹ f1 æ®‹å·®ç›¸åŠ 
      3) æœ€åè¾“å‡ºèåˆåçš„ç‰¹å¾

    æ³¨æ„ï¼š
      - embed_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤
      - è‹¥ f1/f2 çš„ shape = (batch, seq_len, feature_dim)
        åˆ™ embed_dim = feature_dimï¼Œnum_heads * head_dim = embed_dim
    """

    def __init__(self, in_dim, out_dim, num_heads=1):
        super(SingleCoreResidualFusion, self).__init__()
        # f1 ä½œä¸ºâ€œæ ¸å¿ƒâ€ï¼Œåªåšä¸€æ¬¡ Cross-Attention: Query=f1, Key=Value=f2
        # batch_first=True => è¾“å…¥/è¾“å‡º (batch, seq_len, embed_dim)
        self.cross_attn = nn.MultiheadAttention(
            embed_dim=in_dim,
            num_heads=num_heads,
            batch_first=True
        )
        # å°†æ®‹å·®åçš„ f1 åšçº¿æ€§å˜æ¢
        self.fc = nn.Linear(in_dim, out_dim)

    def forward(self, f1, f2):
        """
        f1, f2: shape=(batch_size, seq_len, in_dim)
        è¿”å›: shape=(batch_size, seq_len, out_dim)
        """
        # 1) Cross-Attention: f1 (query) ä» f2 (key=value) è·å–ä¿¡æ¯
        #    attn_f1 çš„ shape ä¸ f1 ä¸€è‡´ (batch, seq_len, in_dim)
        attn_f1, _ = self.cross_attn(query=f1, key=f2, value=f2)

        # 2) æ®‹å·®è¿æ¥ï¼šf1 + attn_f1
        fused_f1 = attn_f1 + f1

        # 3) å…¨è¿æ¥å±‚æˆ–å…¶ä»–åç»­å¤„ç†
        out = self.fc(fused_f1)  # (batch, seq_len, out_dim)
        return out


class ResidualFusion(nn.Module):
    def __init__(self, in_dim, out_dim):
        super(ResidualFusion, self).__init__()
        self.attention1 = nn.MultiheadAttention(embed_dim=in_dim, num_heads=5, batch_first=True)
        self.attention2 = nn.MultiheadAttention(embed_dim=in_dim, num_heads=5, batch_first=True)
        self.fc = nn.Linear(in_dim * 2, out_dim)

    def forward(self, f1, f2):
        # é€šè¿‡è‡ªæ³¨æ„åŠ›æœºåˆ¶å¤„ç†ä¸¤ä¸ªç‰¹å¾
        attn_f1, _ = self.attention1(f1, f2, f2)
        attn_f2, _ = self.attention2(f2, f1, f1)

        # æ®‹å·®è¿æ¥ï¼šå°†æ³¨æ„åŠ›åçš„ç‰¹å¾ä¸åŸå§‹ç‰¹å¾ç›¸åŠ 
        fused_f1 = attn_f1 + f1
        fused_f2 = attn_f2 + f2

        # åˆå¹¶æ®‹å·®åçš„ç‰¹å¾
        combined = torch.cat((fused_f1, fused_f2), dim=-1)

        # é€šè¿‡å…¨è¿æ¥å±‚ç”Ÿæˆæœ€ç»ˆè¾“å‡º
        out = self.fc(combined)
        return out


import torch
import torch.nn as nn

class CAB(nn.Module):
    """é€šé“æ³¨æ„åŠ›æ¨¡å—ï¼ˆé€‚ç”¨äº1Då·ç§¯ï¼‰"""
    def __init__(self, channels, reduction=16):
        super(CAB, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(1)
        self.max_pool = nn.AdaptiveMaxPool1d(1)

        self.fc = nn.Sequential(
            nn.Conv1d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv1d(channels // reduction, channels, 1, bias=False),
            nn.Sigmoid()
        )

    def forward(self, x):
        avg_out = self.fc(self.avg_pool(x))
        max_out = self.fc(self.max_pool(x))
        out = avg_out + max_out
        return x * out

class SAB(nn.Module):
    """ç©ºé—´ï¼ˆæ—¶é—´ï¼‰æ³¨æ„åŠ›æ¨¡å—ï¼ˆé€‚ç”¨äº1Då·ç§¯ï¼‰"""
    def __init__(self, kernel_size=7):
        super(SAB, self).__init__()
        padding = (kernel_size - 1) // 2
        self.conv = nn.Conv1d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        scale = self.sigmoid(self.conv(x_cat))
        return x * scale

class MSCB(nn.Module):
    """å¤šå°ºåº¦å·ç§¯æ¨¡å—ï¼ˆé€‚ç”¨äº1Då·ç§¯ï¼‰"""
    def __init__(self, in_channels, out_channels, kernel_sizes=[3,5,7]):
        super(MSCB, self).__init__()
        self.convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv1d(in_channels, out_channels, k, padding=k//2, groups=in_channels, bias=False),
                nn.BatchNorm1d(out_channels),
                nn.ReLU(inplace=True)
            ) for k in kernel_sizes
        ])
        self.pointwise = nn.Sequential(
            nn.Conv1d(len(kernel_sizes)*out_channels, out_channels, 1, bias=False),
            nn.BatchNorm1d(out_channels),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        out = torch.cat([conv(x) for conv in self.convs], dim=1)
        out = self.pointwise(out)
        return out


class EnhancedTemporalLearner(nn.Module):
    """
    èåˆå¤šå°ºåº¦ä¸æ³¨æ„åŠ›æœºåˆ¶çš„1Då·ç§¯ç‰¹å¾æå–æ¨¡å—ï¼ˆè¾“å‡ºç»´åº¦ä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼‰
    è¾“å…¥: (B, T, 1)
    è¾“å‡º: (B, out_channels * num_kernels)
    """

    def __init__(self, kernel_sizes=[3, 5, 7], out_channels=50):
        super(EnhancedTemporalLearner, self).__init__()
        self.initial_conv = nn.Conv1d(1, out_channels, 1)
        self.cab = CAB(out_channels)
        self.sab = SAB()

        # æ³¨æ„è¿™é‡Œä¿®æ”¹ MSCB è¾“å‡ºé€šé“æ•°
        self.mscb = MSCB(out_channels, out_channels, kernel_sizes)

        # BNå’ŒReLUå±‚ä¿®æ”¹ä¸ºå¯¹åº”çš„è¾“å‡ºç»´åº¦ï¼ˆout_channels * num_kernelsï¼‰
        self.bn = nn.BatchNorm1d(out_channels * len(kernel_sizes))
        self.relu = nn.ReLU()

    def forward(self, x):
        # x shape: (B, T, 1)
        x = x.transpose(1, 2)  # (B, 1, T)
        x = self.initial_conv(x)  # (B, out_channels, T)
        x = self.cab(x)  # é€šé“æ³¨æ„åŠ› (B, out_channels, T)
        x = self.sab(x)  # ç©ºé—´æ³¨æ„åŠ› (B, out_channels, T)

        # æ”¹ä¸ºç›´æ¥æ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾
        multi_scale_feats = [conv(x) for conv in self.mscb.convs]  # æ¯ä¸ªå°ºåº¦ (B,out_channels,T)
        x_cat = torch.cat(multi_scale_feats, dim=1)  # æ‹¼æ¥ (B,150,T)

        x = self.bn(x_cat)
        x = self.relu(x)
        out = x.mean(dim=2)  # å¹³å‡æ± åŒ– (B, 150)
        return out


def segment_eeg_sliding(eeg_array, window_size=8, step=4):
    """
    å¯¹è¾“å…¥ EEG æ•°æ®ï¼ˆå½¢çŠ¶: (62, T, 5)ï¼‰ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼š
      æ¯ä¸ªçª—å£çš„å½¢çŠ¶ä¸º (62, window_size, 5)
      è¿”å›çª—å£åˆ—è¡¨ï¼ˆé€šè¿‡å‡å°æ­¥é•¿å¢å¤§çª—å£é‡å ç‡ï¼Œä¿è¯ä¸ä¸¢å¤±æ•°æ®ï¼‰
    """
    channels, T, f = eeg_array.shape
    segments = []
    for start in range(0, T - window_size + 1, step):
        seg = eeg_array[:, start:start+window_size, :]
        segments.append(seg)
    return segments
# -------------------------------------------
# æ•°æ®åŠ è½½å‡½æ•°ï¼šload_data_inde
# -------------------------------------------

def load_data_inde1(path, subject):
    """
    Independentå®éªŒæ•°æ®åŠ è½½ï¼š
      - æŒ‡å®š subject å¯¹åº”çš„ .mat æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†ï¼Œ
      - ç›®å½•ä¸‹å…¶ä»–æ‰€æœ‰ .mat æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ã€‚
    å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
    å¯¹æ¯ä¸ª trialï¼Œå…ˆé‡‡ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆçª—å£å¤§å°ä¸º16ï¼Œæ­¥é•¿ä¸º4ï¼Œå³çª—å£é‡å è¾ƒå¤§ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªçª—å£ä½¿ç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡ºä¸º (62, 5) çš„è¡¨ç¤ºï¼ˆæ¯ä¸ªé¢‘æ®µæå–1ä¸ªç‰¹å¾ï¼‰ã€‚
    æ ‡ç­¾æŒ‰ label_seed4[1] æå–ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
    """
    # å®šä¹‰ TemporalLearner å’Œèåˆå±‚ï¼Œç”¨äºå°†å¤šå°ºåº¦å·ç§¯è¾“å‡ºé™ç»´ä¸º 1 ç»´
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50 * 3, 1)  # è¾“å‡ºç»´åº¦ä¸º 1
    # å›ºå®šä¸º eval æ¨¡å¼ï¼Œä¸éœ€è¦æ¢¯åº¦
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed4 = [
            [1, 2, 3, 0, 2, 0, 0, 1, 0, 1, 2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3],
            [2, 1, 3, 0, 0, 2, 0, 2, 3, 3, 2, 3, 2, 0, 1, 1, 2, 1, 0, 3, 0, 1, 3, 1],
            [1, 2, 2, 1, 3, 3, 3, 1, 1, 2, 1, 0, 2, 3, 3, 0, 2, 3, 0, 0, 2, 0, 1, 0]
        ]
        data = loadmat(mat_path)
        samples_list = []
        labels_list = []
        # å¯¹æ¯ä¸ª trial (1~24)
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        for i in range(1, 25):
            prefix = keys[0].split("_")[0]  # å¦‚ "ha
            key = f"{prefix}_eeg{i}"

            if key in data:
                trial_data = data[key].astype(np.float32)
                print(key)# åŸå§‹å½¢çŠ¶: (62, T, 5)
                print(data[key].shape)
                # æ›¿æ¢åŸæ¥çš„ segment_eeg_sliding è°ƒç”¨é€»è¾‘
                actual_window_size = min(trial_data.shape[1], 16)
                windows = segment_eeg_sliding(trial_data, window_size=actual_window_size, step=4)
                # å¯¹æ¯ä¸ªçª—å£æå–æ—¶åºç‰¹å¾
                for window in windows:
                    # window: (62, window_size, 5)
                    with torch.no_grad():
                        window_tensor = torch.from_numpy(window)  # (62, window_size, 5)
                        channel_features = []  # æ¯ä¸ªé¢‘æ®µæå–ç‰¹å¾ï¼Œç›®æ ‡è¾“å‡º (62, 1)
                        for f in range(5):
                            # å–å‡ºç¬¬ f ä¸ªé¢‘æ®µï¼šå½¢çŠ¶ (62, window_size)
                            freq_data = window_tensor[:, :, f]
                            # è°ƒæ•´ä¸º (62, window_size, 1)
                            freq_data = freq_data.unsqueeze(-1)
                            # TemporalLearner æå–ç‰¹å¾ï¼Œè¾“å‡º (62, 50*3)
                            temp_feat = temporal_learner(freq_data)
                            # é€šè¿‡èåˆå±‚å°†ç‰¹å¾é™ä¸º (62, 1)
                            feat_reduced = fusion_fc(temp_feat)
                            channel_features.append(feat_reduced)
                        # æ‹¼æ¥å¾—åˆ°çª—å£æ ·æœ¬ï¼Œå½¢çŠ¶ (62, 5)
                        window_feature = torch.cat(channel_features, dim=-1)
                        window_feature = window_feature.cpu().numpy()
                    samples_list.append(window_feature)
                    # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                    labels_list.append(label_seed4[1][i - 1])
        if not samples_list:
            raise ValueError(f"No valid trials found in {mat_path}")
        # åˆå¹¶æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œå¾—åˆ° (n_windows, 62, 5)
        samples_arr = np.stack(samples_list)
        labels_arr = np.array(labels_list, dtype=np.int32)
        return samples_arr, labels_arr

    train_samples, train_labels = [], []
    test_samples, test_labels = [], []
    for filename in os.listdir(path):
        if not filename.endswith('.mat'):
            continue
        mat_path = os.path.join(path, filename)
        if filename == subject:
            x_ts, y_ts = parse_mat_file(mat_path)
            # ä½¿ç”¨ standardize æ ‡å‡†åŒ–ï¼Œè½¬æ¢ä¸º Tensorï¼Œå†è½¬ä¸º numpy
            x_ts = standardize(x_ts).numpy()
            test_samples.append(x_ts)
            test_labels.append(y_ts)
        else:
            print(mat_path)
            x_tr, y_tr = parse_mat_file(mat_path)
            x_tr = standardize(x_tr).numpy()
            train_samples.append(x_tr)
            train_labels.append(y_tr)
    if not test_samples:
        raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•è¢«è¯•æ–‡ä»¶ {subject} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ .mat æ–‡ä»¶")
    if train_samples:
        x_tr_merged = np.concatenate(train_samples, axis=0)
        y_tr_merged = np.concatenate(train_labels, axis=0)
    else:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°é™¤æµ‹è¯•è¢«è¯•å¤–çš„ä»»ä½• .mat æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
    x_ts_merged = np.concatenate(test_samples, axis=0)
    y_ts_merged = np.concatenate(test_labels, axis=0)
    print(f"ç‹¬ç«‹å®éªŒ: è®­ç»ƒæ ·æœ¬æ€»æ•° {x_tr_merged.shape[0]}, æµ‹è¯•æ ·æœ¬æ€»æ•° {x_ts_merged.shape[0]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {x_tr_merged.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_ts_merged.shape}")
    return {
        "x_tr": x_tr_merged,
        "y_tr": y_tr_merged,
        "x_ts": x_ts_merged,
        "y_ts": y_ts_merged
    }

def load_data_inde2(path, subject):
    """
    Independentå®éªŒæ•°æ®åŠ è½½ï¼š
      - æŒ‡å®š subject å¯¹åº”çš„ .mat æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†ï¼Œ
      - ç›®å½•ä¸‹å…¶ä»–æ‰€æœ‰ .mat æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ã€‚
    å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
    å¯¹æ¯ä¸ª trialï¼Œå…ˆé‡‡ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆçª—å£å¤§å°ä¸º16ï¼Œæ­¥é•¿ä¸º4ï¼Œå³çª—å£é‡å è¾ƒå¤§ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªçª—å£ä½¿ç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡ºä¸º (62, 5) çš„è¡¨ç¤ºï¼ˆæ¯ä¸ªé¢‘æ®µæå–1ä¸ªç‰¹å¾ï¼‰ã€‚
    æ ‡ç­¾æŒ‰ label_seed4[1] æå–ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
    """
    # å®šä¹‰ TemporalLearner å’Œèåˆå±‚ï¼Œç”¨äºå°†å¤šå°ºåº¦å·ç§¯è¾“å‡ºé™ç»´ä¸º 1 ç»´
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50 * 3, 1)  # è¾“å‡ºç»´åº¦ä¸º 1
    # å›ºå®šä¸º eval æ¨¡å¼ï¼Œä¸éœ€è¦æ¢¯åº¦
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed = [
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0]
        ]
        data = loadmat(mat_path)
        samples_list = []
        labels_list = []
        # å¯¹æ¯ä¸ª trial (1~24)
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        for i in range(1, 16):
            prefix = keys[0].split("_")[0]  # å¦‚ "ha
            key = f"{prefix}_eeg{i}"

            if key in data:
                trial_data = data[key].astype(np.float32)
                print(key)# åŸå§‹å½¢çŠ¶: (62, T, 5)
                print(data[key].shape)
                # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œæ­¥é•¿è®¾ç½®ä¸º 4 å¢å¤§é‡å 
                actual_window_size = min(trial_data.shape[1], 16)
                windows = segment_eeg_sliding(trial_data, window_size=actual_window_size, step=4)
                # å¯¹æ¯ä¸ªçª—å£æå–æ—¶åºç‰¹å¾
                for window in windows:
                    # window: (62, window_size, 5)
                    with torch.no_grad():
                        window_tensor = torch.from_numpy(window)  # (62, window_size, 5)
                        channel_features = []  # æ¯ä¸ªé¢‘æ®µæå–ç‰¹å¾ï¼Œç›®æ ‡è¾“å‡º (62, 1)
                        for f in range(5):
                            # å–å‡ºç¬¬ f ä¸ªé¢‘æ®µï¼šå½¢çŠ¶ (62, window_size)
                            freq_data = window_tensor[:, :, f]
                            # è°ƒæ•´ä¸º (62, window_size, 1)
                            freq_data = freq_data.unsqueeze(-1)
                            # TemporalLearner æå–ç‰¹å¾ï¼Œè¾“å‡º (62, 50*3)
                            temp_feat = temporal_learner(freq_data)
                            # é€šè¿‡èåˆå±‚å°†ç‰¹å¾é™ä¸º (62, 1)
                            feat_reduced = fusion_fc(temp_feat)
                            channel_features.append(feat_reduced)
                        # æ‹¼æ¥å¾—åˆ°çª—å£æ ·æœ¬ï¼Œå½¢çŠ¶ (62, 5)
                        window_feature = torch.cat(channel_features, dim=-1)
                        window_feature = window_feature.cpu().numpy()
                    samples_list.append(window_feature)
                    # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                    labels_list.append(label_seed[1][i - 1])
        if not samples_list:
            raise ValueError(f"No valid trials found in {mat_path}")
        # åˆå¹¶æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œå¾—åˆ° (n_windows, 62, 5)
        samples_arr = np.stack(samples_list)
        labels_arr = np.array(labels_list, dtype=np.int32)
        return samples_arr, labels_arr

    train_samples, train_labels = [], []
    test_samples, test_labels = [], []
    for filename in os.listdir(path):
        if not filename.endswith('.mat'):
            continue
        mat_path = os.path.join(path, filename)
        if filename == subject:
            x_ts, y_ts = parse_mat_file(mat_path)
            # ä½¿ç”¨ standardize æ ‡å‡†åŒ–ï¼Œè½¬æ¢ä¸º Tensorï¼Œå†è½¬ä¸º numpy
            x_ts = standardize(x_ts).numpy()
            test_samples.append(x_ts)
            test_labels.append(y_ts)
        else:
            print(mat_path)
            x_tr, y_tr = parse_mat_file(mat_path)
            x_tr = standardize(x_tr).numpy()
            train_samples.append(x_tr)
            train_labels.append(y_tr)
    if not test_samples:
        raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•è¢«è¯•æ–‡ä»¶ {subject} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ .mat æ–‡ä»¶")
    if train_samples:
        x_tr_merged = np.concatenate(train_samples, axis=0)
        y_tr_merged = np.concatenate(train_labels, axis=0)
    else:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°é™¤æµ‹è¯•è¢«è¯•å¤–çš„ä»»ä½• .mat æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
    x_ts_merged = np.concatenate(test_samples, axis=0)
    y_ts_merged = np.concatenate(test_labels, axis=0)
    print(f"ç‹¬ç«‹å®éªŒ: è®­ç»ƒæ ·æœ¬æ€»æ•° {x_tr_merged.shape[0]}, æµ‹è¯•æ ·æœ¬æ€»æ•° {x_ts_merged.shape[0]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {x_tr_merged.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_ts_merged.shape}")
    return {
        "x_tr": x_tr_merged,
        "y_tr": y_tr_merged,
        "x_ts": x_ts_merged,
        "y_ts": y_ts_merged
    }


def load_data_inde3(path, subject):
    print(subject)
    """
    Independentå®éªŒæ•°æ®åŠ è½½ï¼š
      - æŒ‡å®š subject å¯¹åº”çš„ .mat æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†ï¼Œ
      - ç›®å½•ä¸‹å…¶ä»–æ‰€æœ‰ .mat æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ã€‚
    å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
    å¯¹æ¯ä¸ª trialï¼Œå…ˆé‡‡ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆçª—å£å¤§å°ä¸º16ï¼Œæ­¥é•¿ä¸º4ï¼Œå³çª—å£é‡å è¾ƒå¤§ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªçª—å£ä½¿ç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡ºä¸º (62, 5) çš„è¡¨ç¤ºï¼ˆæ¯ä¸ªé¢‘æ®µæå–1ä¸ªç‰¹å¾ï¼‰ã€‚
    æ ‡ç­¾æŒ‰ label_seed4[1] æå–ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
    """
    # å®šä¹‰ TemporalLearner å’Œèåˆå±‚ï¼Œç”¨äºå°†å¤šå°ºåº¦å·ç§¯è¾“å‡ºé™ç»´ä¸º 1 ç»´
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50 * 3, 1)  # è¾“å‡ºç»´åº¦ä¸º 1
    # å›ºå®šä¸º eval æ¨¡å¼ï¼Œä¸éœ€è¦æ¢¯åº¦
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed = [
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
        ]
        data = loadmat(mat_path)
        samples_list = []
        labels_list = []
        # å¯¹æ¯ä¸ª trial (1~24)
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        for i in range(1, 29):
            key = f"session_{i:03d}"
            if key in data:
                trial_data = data[key].astype(np.float32)
                print(key)# åŸå§‹å½¢çŠ¶: (62, T, 5)
                print(data[key].shape)
                # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œæ­¥é•¿è®¾ç½®ä¸º 4 å¢å¤§é‡å 
                actual_window_size = min(trial_data.shape[1], 16)
                windows = segment_eeg_sliding(trial_data, window_size=actual_window_size, step=4)
                # å¯¹æ¯ä¸ªçª—å£æå–æ—¶åºç‰¹å¾
                for window in windows:
                    # window: (62, window_size, 5)
                    with torch.no_grad():
                        window_tensor = torch.from_numpy(window)  # (62, window_size, 5)
                        channel_features = []  # æ¯ä¸ªé¢‘æ®µæå–ç‰¹å¾ï¼Œç›®æ ‡è¾“å‡º (62, 1)
                        for f in range(5):
                            # å–å‡ºç¬¬ f ä¸ªé¢‘æ®µï¼šå½¢çŠ¶ (62, window_size)
                            freq_data = window_tensor[:, :, f]
                            # è°ƒæ•´ä¸º (62, window_size, 1)
                            freq_data = freq_data.unsqueeze(-1)
                            # TemporalLearner æå–ç‰¹å¾ï¼Œè¾“å‡º (62, 50*3)
                            temp_feat = temporal_learner(freq_data)
                            # é€šè¿‡èåˆå±‚å°†ç‰¹å¾é™ä¸º (62, 1)
                            feat_reduced = fusion_fc(temp_feat)
                            channel_features.append(feat_reduced)
                        # æ‹¼æ¥å¾—åˆ°çª—å£æ ·æœ¬ï¼Œå½¢çŠ¶ (62, 5)
                        window_feature = torch.cat(channel_features, dim=-1)
                        window_feature = window_feature.cpu().numpy()
                    samples_list.append(window_feature)
                    # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                    labels_list.append(label_seed[1][i - 1])
        if not samples_list:
            raise ValueError(f"No valid trials found in {mat_path}")
        # åˆå¹¶æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œå¾—åˆ° (n_windows, 62, 5)
        samples_arr = np.stack(samples_list)
        labels_arr = np.array(labels_list, dtype=np.int32)
        return samples_arr, labels_arr

    train_samples, train_labels = [], []
    test_samples, test_labels = [], []
    for filename in os.listdir(path):
        if not filename.endswith('.mat'):
            continue
        mat_path = os.path.join(path, filename)
        if filename == subject:
            x_ts, y_ts = parse_mat_file(mat_path)
            # ä½¿ç”¨ standardize æ ‡å‡†åŒ–ï¼Œè½¬æ¢ä¸º Tensorï¼Œå†è½¬ä¸º numpy
            x_ts = standardize(x_ts).numpy()
            test_samples.append(x_ts)
            test_labels.append(y_ts)
        else:
            print(mat_path)
            x_tr, y_tr = parse_mat_file(mat_path)
            x_tr = standardize(x_tr).numpy()
            train_samples.append(x_tr)
            train_labels.append(y_tr)
    if not test_samples:
        raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•è¢«è¯•æ–‡ä»¶ {subject} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ .mat æ–‡ä»¶")
    if train_samples:
        x_tr_merged = np.concatenate(train_samples, axis=0)
        y_tr_merged = np.concatenate(train_labels, axis=0)
    else:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°é™¤æµ‹è¯•è¢«è¯•å¤–çš„ä»»ä½• .mat æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
    x_ts_merged = np.concatenate(test_samples, axis=0)
    y_ts_merged = np.concatenate(test_labels, axis=0)
    print(f"ç‹¬ç«‹å®éªŒ: è®­ç»ƒæ ·æœ¬æ€»æ•° {x_tr_merged.shape[0]}, æµ‹è¯•æ ·æœ¬æ€»æ•° {x_ts_merged.shape[0]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {x_tr_merged.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_ts_merged.shape}")
    return {
        "x_tr": x_tr_merged,
        "y_tr": y_tr_merged,
        "x_ts": x_ts_merged,
        "y_ts": y_ts_merged
    }
def load_data_inde_yuan(path, subject):
    """
    Independentå®éªŒæ•°æ®åŠ è½½ï¼š
      - æŒ‡å®š subject å¯¹åº”çš„ .mat æ–‡ä»¶ä½œä¸ºæµ‹è¯•é›†ï¼Œ
      - ç›®å½•ä¸‹å…¶ä»–æ‰€æœ‰ .mat æ–‡ä»¶ä½œä¸ºè®­ç»ƒé›†ã€‚
    å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
    å¯¹æ¯ä¸ª trialï¼Œå…ˆé‡‡ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆçª—å£å¤§å°ä¸º16ï¼Œæ­¥é•¿ä¸º4ï¼Œå³çª—å£é‡å è¾ƒå¤§ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªçª—å£ä½¿ç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡ºä¸º (62, 5) çš„è¡¨ç¤ºï¼ˆæ¯ä¸ªé¢‘æ®µæå–1ä¸ªç‰¹å¾ï¼‰ã€‚
    æ ‡ç­¾æŒ‰ label_seed4[1] æå–ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
    """
    # å®šä¹‰ TemporalLearner å’Œèåˆå±‚ï¼Œç”¨äºå°†å¤šå°ºåº¦å·ç§¯è¾“å‡ºé™ç»´ä¸º 1 ç»´
    temporal_learner = TemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50 * 3, 1)  # è¾“å‡ºç»´åº¦ä¸º 1
    # å›ºå®šä¸º eval æ¨¡å¼ï¼Œä¸éœ€è¦æ¢¯åº¦
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed4 = [
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0]
        ]
        data = loadmat(mat_path)
        samples_list = []
        labels_list = []
        # å¯¹æ¯ä¸ª trial (1~15)
        for i in range(1, 16):
            key = f'de_LDS{i}'
            if key in data:
                trial_data = data[key].astype(np.float32)  # åŸå§‹å½¢çŠ¶: (62, T, 5)
                # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œæ­¥é•¿è®¾ç½®ä¸º 4 å¢å¤§é‡å ç‡
                windows = segment_eeg_sliding(trial_data, window_size=16, step=4)
                # å¯¹æ¯ä¸ªçª—å£æå–æ—¶åºç‰¹å¾
                for window in windows:
                    # window: (62, window_size, 5)
                    with torch.no_grad():
                        window_tensor = torch.from_numpy(window)  # (62, window_size, 5)
                        channel_features = []  # æ¯ä¸ªé¢‘æ®µæå–ç‰¹å¾ï¼Œç›®æ ‡è¾“å‡º (62, 1)
                        for f in range(5):
                            # å–å‡ºç¬¬ f ä¸ªé¢‘æ®µï¼šå½¢çŠ¶ (62, window_size)
                            freq_data = window_tensor[:, :, f]
                            # è°ƒæ•´ä¸º (62, window_size, 1)
                            freq_data = freq_data.unsqueeze(-1)
                            # TemporalLearner æå–ç‰¹å¾ï¼Œè¾“å‡º (62, 50*3)
                            temp_feat = temporal_learner(freq_data)
                            # é€šè¿‡èåˆå±‚å°†ç‰¹å¾é™ä¸º (62, 1)
                            feat_reduced = fusion_fc(temp_feat)
                            channel_features.append(feat_reduced)
                        # æ‹¼æ¥å¾—åˆ°çª—å£æ ·æœ¬ï¼Œå½¢çŠ¶ (62, 5)
                        window_feature = torch.cat(channel_features, dim=-1)
                        window_feature = window_feature.cpu().numpy()
                    samples_list.append(window_feature)
                    # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                    labels_list.append(label_seed4[0][i - 1])
        if not samples_list:
            raise ValueError(f"No valid trials found in {mat_path}")
        # åˆå¹¶æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œå¾—åˆ° (n_windows, 62, 5)
        samples_arr = np.stack(samples_list)
        labels_arr = np.array(labels_list, dtype=np.int32)
        return samples_arr, labels_arr

    train_samples, train_labels = [], []
    test_samples, test_labels = [], []
    for filename in os.listdir(path):
        if not filename.endswith('.mat'):
            continue
        mat_path = os.path.join(path, filename)
        if filename == subject:
            x_ts, y_ts = parse_mat_file(mat_path)
            # ä½¿ç”¨ standardize æ ‡å‡†åŒ–ï¼Œè½¬æ¢ä¸º Tensorï¼Œå†è½¬ä¸º numpy
            x_ts = standardize(x_ts).numpy()
            test_samples.append(x_ts)
            test_labels.append(y_ts)
        else:
            x_tr, y_tr = parse_mat_file(mat_path)
            x_tr = standardize(x_tr).numpy()
            train_samples.append(x_tr)
            train_labels.append(y_tr)
    if not test_samples:
        raise ValueError(f"æŒ‡å®šçš„æµ‹è¯•è¢«è¯•æ–‡ä»¶ {subject} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ .mat æ–‡ä»¶")
    if train_samples:
        x_tr_merged = np.concatenate(train_samples, axis=0)
        y_tr_merged = np.concatenate(train_labels, axis=0)
    else:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°é™¤æµ‹è¯•è¢«è¯•å¤–çš„ä»»ä½• .mat æ–‡ä»¶ç”¨äºè®­ç»ƒï¼Œè¯·æ£€æŸ¥ç›®å½•ã€‚")
    x_ts_merged = np.concatenate(test_samples, axis=0)
    y_ts_merged = np.concatenate(test_labels, axis=0)
    print(f"ç‹¬ç«‹å®éªŒ: è®­ç»ƒæ ·æœ¬æ€»æ•° {x_tr_merged.shape[0]}, æµ‹è¯•æ ·æœ¬æ€»æ•° {x_ts_merged.shape[0]}")
    print(f"è®­ç»ƒé›†å½¢çŠ¶: {x_tr_merged.shape}, æµ‹è¯•é›†å½¢çŠ¶: {x_ts_merged.shape}")
    return {
        "x_tr": x_tr_merged,
        "y_tr": y_tr_merged,
        "x_ts": x_ts_merged,
        "y_ts": y_ts_merged
    }
def load_data_denpendent(path, subject):
    """
    Independentå®éªŒæ•°æ®åŠ è½½ï¼š
      - æ‰€æœ‰çš„ .mat æ–‡ä»¶ä½œä¸ºåŒä¸€æ•°æ®é›†ï¼Œä¸åŒºåˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
    å¯¹æ¯ä¸ª trialï¼Œå…ˆé‡‡ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆçª—å£å¤§å°ä¸º16ï¼Œæ­¥é•¿ä¸º4ï¼Œå³çª—å£é‡å è¾ƒå¤§ï¼‰ï¼Œ
    ç„¶åå¯¹æ¯ä¸ªçª—å£ä½¿ç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡ºä¸º (62, 5) çš„è¡¨ç¤ºï¼ˆæ¯ä¸ªé¢‘æ®µæå–1ä¸ªç‰¹å¾ï¼‰ã€‚
    æ ‡ç­¾æŒ‰ label_seed4[1] æå–ï¼ˆ4åˆ†ç±»ï¼š0,1,2,3ï¼‰ã€‚
    """
    # å®šä¹‰ TemporalLearner å’Œèåˆå±‚ï¼Œç”¨äºå°†å¤šå°ºåº¦å·ç§¯è¾“å‡ºé™ç»´ä¸º 1 ç»´
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5,7], out_channels=50)
    fusion_fc = torch.nn.Linear(50 * 3, 1)  # è¾“å‡ºç»´åº¦ä¸º 1
    # å›ºå®šä¸º eval æ¨¡å¼ï¼Œä¸éœ€è¦æ¢¯åº¦
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed4 = [
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0],
            [2, 1, 0, 0, 1, 2, 0, 1, 2, 2, 1, 0, 1, 2, 0]
        ]
        data = loadmat(mat_path)
        train_samples, train_labels = [], []
        test_samples, test_labels = [], []
        # å¯¹æ¯ä¸ª trial (1~24)
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]

        for i in range(1, 16):
            # key = f'de_LDS{i}'
            prefix = keys[0].split("_")[0]  # å¦‚ "ha
            key = f"{prefix}_eeg{i}"
            if key in data:
                print(key)
                print(data[key].shape)
                trial_data = data[key].astype(np.float32)  # åŸå§‹å½¢çŠ¶: (62, T, 5)
                # ä½¿ç”¨æ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼Œæ­¥é•¿è®¾ç½®ä¸º 4 å¢å¤§é‡å ç‡
                windows = segment_eeg_sliding(trial_data, window_size=16, step=4)
                # å¯¹æ¯ä¸ªçª—å£æå–æ—¶åºç‰¹å¾
                for window in windows:
                    # window: (62, window_size, 5)
                    with torch.no_grad():
                        window_tensor = torch.from_numpy(window)  # (62, window_size, 5)
                        channel_features = []  # æ¯ä¸ªé¢‘æ®µæå–ç‰¹å¾ï¼Œç›®æ ‡è¾“å‡º (62, 1)
                        for f in range(5):
                            # å–å‡ºç¬¬ f ä¸ªé¢‘æ®µï¼šå½¢çŠ¶ (62, window_size)
                            freq_data = window_tensor[:, :, f]
                            # è°ƒæ•´ä¸º (62, window_size, 1)
                            freq_data = freq_data.unsqueeze(-1)
                            # TemporalLearner æå–ç‰¹å¾ï¼Œè¾“å‡º (62, 50*3)
                            temp_feat = temporal_learner(freq_data)
                            # é€šè¿‡èåˆå±‚å°†ç‰¹å¾é™ä¸º (62, 1)

                            feat_reduced = fusion_fc(temp_feat)
                            channel_features.append(feat_reduced)
                        # æ‹¼æ¥å¾—åˆ°çª—å£æ ·æœ¬ï¼Œå½¢çŠ¶ (62, 5)
                        window_feature = torch.cat(channel_features, dim=-1)
                        window_feature = window_feature.cpu().numpy()
                    if i<=12:
                       train_samples.append(window_feature)
                    # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                       train_labels.append(label_seed4[0][i - 1])
                    else:
                       test_samples.append(window_feature)
                        # å¯¹äºæ¯ä¸ªçª—å£ï¼Œæ ‡ç­¾å– trial å¯¹åº”çš„æ ‡ç­¾ï¼š label_seed4[1][i-1]
                       test_labels.append(label_seed4[0][i - 1])
        if not train_samples:
            raise ValueError(f"No valid trials found in {mat_path}")
        # åˆå¹¶æ‰€æœ‰çª—å£æ ·æœ¬ï¼Œå¾—åˆ° (n_windows, 62, 5)
        samples_tr = np.stack(train_samples)
        labels_tr = np.array(train_labels, dtype=np.int32)
        samples_ts = np.stack(test_samples)
        labels_ts = np.array(test_labels, dtype=np.int32)
        return samples_tr, labels_tr,samples_ts,labels_ts
        # æ‰“ä¹±

    mat_path = os.path.join(path, subject)
    x_tr, y_tr,x_ts, y_ts = parse_mat_file(mat_path)
    x_tr = standardize(x_tr)
    x_ts=standardize(x_ts)

    print(f"è®­ç»ƒé›†å¤§å°: {x_tr.shape[0]}, æµ‹è¯•é›†å¤§å°: {x_ts.shape[0]}")
    # x_tr = np.expand_dims(x_tr, axis=2)
    # x_ts = np.expand_dims(x_ts, axis=2)
    print(x_tr.shape)
    print(y_tr.shape)
    print(x_ts.shape)
    print(y_ts.shape)

    # 6. è¿”å›ç»“æœ
    return {
        "x_tr": x_tr.detach().numpy(),  # shape: (N_train, 62, 5)
        "y_tr": y_tr,
        "x_ts": x_ts.detach().numpy(),  # shape: (N_test, 62, 5)
        "y_ts": y_ts
    }

import os
import torch
import numpy as np
from scipy.io import loadmat

# def load_data_denpendent2(path, subject):
#     """
#     Independent å®éªŒæ•°æ®åŠ è½½ï¼ˆè·¨ä¼šè¯å®éªŒï¼‰ï¼š
#       - å¯¹äºæ¯ä¸ªæ ‡ç­¾ï¼ˆ0,1,2,3ï¼‰ï¼Œé€‰å–ç¬¬ä¸€ä¸ªå‡ºç°è¯¥æ ‡ç­¾çš„ trial æ•´ä¸ªä¼šè¯ä½œä¸ºæµ‹è¯•ï¼Œ
#         å…¶ä½™ trials ä½œä¸ºè®­ç»ƒã€‚
#       - å‡è®¾æ¯ä¸ª .mat æ–‡ä»¶åŒ…å« 24 ä¸ª trialï¼Œæ¯ä¸ª trial çš„æ•°æ®å½¢çŠ¶ä¸º (62, T, 5)ã€‚
#       - å¯¹æ¯ä¸ª trialï¼Œæ»‘åŠ¨çª—å£åˆ‡åˆ†ï¼ˆwindow_size=16, step=4ï¼‰ï¼Œ
#         ç„¶åç”¨ TemporalLearner æå–æ—¶åºç‰¹å¾ï¼Œè¾“å‡º (62,1)Ã—5â†’(62,5)ã€‚
#       - æ ‡ç­¾ä» label_seed4[0] ä¸­å–ã€‚
#     """
#     # å®šä¹‰ç‰¹å¾æå–æ¨¡å‹ï¼ˆConv1d æ—¶åºç‰¹å¾ï¼‰
#     temporal_learner = TemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
#     fusion_fc = torch.nn.Linear(50 * 3, 1)
#     temporal_learner.eval()
#     fusion_fc.eval()
#
#     def parse_mat_file(mat_path):
#         # é¢„å®šä¹‰çš„ 24 ä¸ª trial æ ‡ç­¾
#         label_seed4 = [
#             [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3],
#             [2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1],
#             [1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]
#         ]
#         labels = label_seed4[2]  # å–ç¬¬ä¸€ç»„æ ‡ç­¾
#         # é€‰å–æ¯ä¸ªæ ‡ç­¾ç¬¬ä¸€æ¬¡å‡ºç°çš„ trial ä½œä¸ºæµ‹è¯•é›†
#         test_trials = []
#         for target_label in range(4):
#             for idx, lbl in enumerate(labels, start=1):
#                 if lbl == target_label:
#                     test_trials.append(idx)
#                     break
#
#         data = loadmat(mat_path)
#         train_samples, train_labels = [], []
#         test_samples,  test_labels  = [], []
#
#         keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
#         prefix = keys[0].split("_")[0] if keys else ""
#
#         for i in range(1, 25):
#             key = f"{prefix}_eeg{i}"
#             if key not in data:
#                 continue
#
#             trial_data = data[key].astype(np.float32)  # (62, T, 5)
#             windows = segment_eeg_sliding(trial_data, window_size=16, step=4)
#
#             for window in windows:
#                 with torch.no_grad():
#                     # window: (62,16,5)
#                     channel_feats = []
#                     wt = torch.from_numpy(window)
#                     for f in range(5):
#                         freq_data = wt[:, :, f].unsqueeze(-1)          # (62,16,1)
#                         tmp = temporal_learner(freq_data)               # (62,150)
#                         red = fusion_fc(tmp)                            # (62,1)
#                         channel_feats.append(red)
#                     feat62x5 = torch.cat(channel_feats, dim=-1).cpu().numpy()  # (62,5)
#
#                 if i in test_trials:
#                     test_samples.append(feat62x5)
#                     test_labels.append(labels[i-1])
#                 else:
#                     train_samples.append(feat62x5)
#                     train_labels.append(labels[i-1])
#
#         if not train_samples:
#             raise ValueError(f"No training windows in {mat_path}")
#         if not test_samples:
#             raise ValueError(f"No testing windows in {mat_path}")
#
#         return (
#             np.stack(train_samples),
#             np.array(train_labels, dtype=np.int32),
#             np.stack(test_samples),
#             np.array(test_labels,  dtype=np.int32),
#         )
#
#     mat_path = os.path.join(path, subject)
#     x_tr, y_tr, x_ts, y_ts = parse_mat_file(mat_path)
#
#     # æ ‡å‡†åŒ–
#     x_tr = standardize(x_tr)
#     x_ts = standardize(x_ts)
#
#     print(f"è®­ç»ƒé›†: samples={x_tr.shape}, labels={y_tr.shape}")
#     print(f"æµ‹è¯•é›†: samples={x_ts.shape}, labels={y_ts.shape}")
#
#     return {
#         "x_tr": x_tr.detach().numpy(),  # (N_train, 62, 5)
#         "y_tr": y_tr,
#         "x_ts": x_ts.detach().numpy(),  # (N_test,  62, 5)
#         "y_ts": y_ts
#     }


def load_data_denpendent3(path, subject, fold_idx=1):
    """
    Dependent è·¨ä¼šè¯6æŠ˜äº¤å‰å®éªŒï¼š
    fold_idx: 0~5
    æ¯ä¸ªæ ‡ç­¾åœ¨24ä¸ªtrialä¸­å„6æ¬¡å‡ºç°ï¼Œå–æ¯ç»„ç¬¬fold_idxä¸ªä¸ºæµ‹è¯•ï¼Œå…¶ä½™ä¸ºè®­ç»ƒ
    """
    # ç‰¹å¾æå–
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50*3,1)
    temporal_learner.eval(); fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed4 = [
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
            [1, 0, 3, 2, 4, 5, 6, 0, 1, 2, 5, 6, 3, 4, 0, 4, 5, 1, 1, 0, 6, 5, 3, 3, 2, 4, 2, 6],
           ]
        labels = label_seed4[2]  # ä½¿ç”¨ç¬¬ä¸€è¡Œæ ‡ç­¾
        # æŒ‰æ ‡ç­¾åˆ†ç»„ç´¢å¼•
        label_indices = {lbl: [] for lbl in set(labels)}
        for idx, lbl in enumerate(labels, start=1):
            label_indices[lbl].append(idx)
        # æ¯ä¸ªæ ‡ç­¾ç¬¬fold_idxæ¬¡å‡ºç°ä¸ºæµ‹è¯•
        test_trials = [label_indices[lbl][fold_idx] for lbl in sorted(label_indices.keys())]

        data = loadmat(mat_path)
        train_samples, train_labels = [], []
        test_samples, test_labels = [], []
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        prefix = keys[0].split("_")[0] if keys else ""

        for i in range(1,29):
            key = f"session_{i:03d}"
            print(key)
            if key not in data: continue
            trial_data = data[key].astype(np.float32)
            actual_window_size = min(trial_data.shape[1], 16)
            windows = segment_eeg_sliding(trial_data, window_size=actual_window_size, step=4)
            for win in windows:
                with torch.no_grad():
                    wt = torch.from_numpy(win)
                    feats = []
                    for f in range(5):
                        tmp = temporal_learner(wt[:,:,f].unsqueeze(-1))
                        red = fusion_fc(tmp)
                        feats.append(red)
                    feat62x5 = torch.cat(feats, dim=-1).cpu().numpy()
                if i in test_trials:
                    test_samples.append(feat62x5)
                    test_labels.append(labels[i-1])
                else:
                    train_samples.append(feat62x5)
                    train_labels.append(labels[i-1])

        if not train_samples or not test_samples:
            raise ValueError("Empty split for subject {} fold {}".format(subject, fold_idx))
        return (
            np.stack(train_samples), np.array(train_labels, np.int32),
            np.stack(test_samples),  np.array(test_labels, np.int32)
        )

    mat_path = os.path.join(path, subject)
    x_tr, y_tr, x_ts, y_ts = parse_mat_file(mat_path)
    x_tr = standardize(x_tr); x_ts = standardize(x_ts)
    return {"x_tr": x_tr.detach().numpy(), "y_tr": y_tr,
            "x_ts": x_ts.detach().numpy(), "y_ts": y_ts}
def load_data_denpendent2(path, subject, fold_idx=1):
    """
    Dependent è·¨ä¼šè¯6æŠ˜äº¤å‰å®éªŒï¼š
    fold_idx: 0~5
    æ¯ä¸ªæ ‡ç­¾åœ¨24ä¸ªtrialä¸­å„6æ¬¡å‡ºç°ï¼Œå–æ¯ç»„ç¬¬fold_idxä¸ªä¸ºæµ‹è¯•ï¼Œå…¶ä½™ä¸ºè®­ç»ƒ
    """
    # ç‰¹å¾æå–
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50*3,1)
    temporal_learner.eval(); fusion_fc.eval()

    def parse_mat_file(mat_path):
        label_seed4 = [
           [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3],
           [2,1,3,0,0,2,0,2,3,3,2,3,2,0,1,1,2,1,0,3,0,1,3,1],
           [1,2,2,1,3,3,3,1,1,2,1,0,2,3,3,0,2,3,0,0,2,0,1,0]
           ]
        labels = label_seed4[2]  # ä½¿ç”¨ç¬¬ä¸€è¡Œæ ‡ç­¾
        # æŒ‰æ ‡ç­¾åˆ†ç»„ç´¢å¼•
        label_indices = {lbl: [] for lbl in set(labels)}
        for idx, lbl in enumerate(labels, start=1):
            label_indices[lbl].append(idx)
        # æ¯ä¸ªæ ‡ç­¾ç¬¬fold_idxæ¬¡å‡ºç°ä¸ºæµ‹è¯•
        test_trials = [label_indices[lbl][fold_idx] for lbl in sorted(label_indices.keys())]

        data = loadmat(mat_path)
        train_samples, train_labels = [], []
        test_samples, test_labels = [], []
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        prefix = keys[0].split("_")[0] if keys else ""

        for i in range(1,25):
            key = f"{prefix}_eeg{i}"
            print(key)
            if key not in data: continue
            trial_data = data[key].astype(np.float32)
            windows = segment_eeg_sliding(trial_data, window_size=16, step=4)
            for win in windows:
                with torch.no_grad():
                    wt = torch.from_numpy(win)
                    feats = []
                    for f in range(5):
                        tmp = temporal_learner(wt[:,:,f].unsqueeze(-1))
                        red = fusion_fc(tmp)
                        feats.append(red)
                    feat62x5 = torch.cat(feats, dim=-1).cpu().numpy()
                if i in test_trials:
                    test_samples.append(feat62x5)
                    test_labels.append(labels[i-1])
                else:
                    train_samples.append(feat62x5)
                    train_labels.append(labels[i-1])

        if not train_samples or not test_samples:
            raise ValueError("Empty split for subject {} fold {}".format(subject, fold_idx))
        return (
            np.stack(train_samples), np.array(train_labels, np.int32),
            np.stack(test_samples),  np.array(test_labels, np.int32)
        )

    mat_path = os.path.join(path, subject)
    x_tr, y_tr, x_ts, y_ts = parse_mat_file(mat_path)
    x_tr = standardize(x_tr); x_ts = standardize(x_ts)
    return {"x_tr": x_tr.detach().numpy(), "y_tr": y_tr,
            "x_ts": x_ts.detach().numpy(), "y_ts": y_ts}

def load_data_denpendent1(path, subject, fold_idx=2):
    """
    Dependent è·¨ä¼šè¯5æŠ˜äº¤å‰å®éªŒ for SEEDï¼š
      - æ¯ä¸ª trial å¯¹åº”ä¸€ä¸ªä¼šè¯ï¼Œæ€»å…± 15 ä¸ªä¼šè¯ï¼Œ3 ç±»æ ‡ç­¾
      - fold_idx: 0~4ï¼›å¯¹æ¯ä¸ªæ ‡ç­¾ï¼Œå–å®ƒåœ¨ 15 ä¸ªä¼šè¯ä¸­ç¬¬ fold_idx æ¬¡å‡ºç°çš„ä¼šè¯åšæµ‹è¯•
      - å…¶ä½™ä¼šè¯å…¨éƒ¨åšè®­ç»ƒ
    è¿”å›:
      {
        "x_tr": (N_train, 62, 5),
        "y_tr": (N_train,),
        "x_ts": (N_test,  62, 5),
        "y_ts": (N_test,)
      }
    """
    # 1) ç‰¹å¾æå–æ¨¡å—ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼‰
    temporal_learner = EnhancedTemporalLearner(kernel_sizes=[3, 5, 7], out_channels=50)
    fusion_fc = torch.nn.Linear(50*3, 1)
    temporal_learner.eval()
    fusion_fc.eval()

    def parse_mat_file(mat_path):
        # 2) SEED 15 ä¼šè¯çš„æ ‡ç­¾åˆ—è¡¨ï¼Œ0/1/2 å„å‡ºç° 5 æ¬¡
        label_sessions = [2,1,0,0,1,2,0,1,2,2,1,0,1,2,0]
        # æ„å»ºç´¢å¼•ï¼šlabel -> ä¼šè¯ç¼–å·åˆ—è¡¨
        label_indices = {lbl: [] for lbl in set(label_sessions)}
        for sess_idx, lbl in enumerate(label_sessions, start=1):
            label_indices[lbl].append(sess_idx)
        # æ¯ä¸ªæ ‡ç­¾å–ç¬¬ fold_idx æ¬¡å‡ºç°çš„ sess_idx ä½œä¸ºæµ‹è¯•é›†
        test_trials = [ label_indices[lbl][fold_idx] for lbl in sorted(label_indices.keys()) ]

        data = loadmat(mat_path)
        train_samples, train_labels = [], []
        test_samples,  test_labels  = [], []
        keys = [k for k in data if k.endswith("eeg1") or k.endswith("eeg01")]
        prefix = keys[0].split("_")[0] if keys else ""

        # 3) æŠŠæ¯ä¸ª trial æ‹†æˆæ»‘åŠ¨çª—å£ï¼Œå†ææ—¶åºç‰¹å¾
        for i in range(1, 16):
            key = f"{prefix}_eeg{i}"
            print(key)
            if key not in data: continue
            trial_data = data[key].astype(np.float32)  # (62, T, 5)
            actual_window_size = min(trial_data.shape[1], 16)
            windows = segment_eeg_sliding(trial_data, window_size=actual_window_size, step=4)
            for win in windows:
                with torch.no_grad():
                    wt = torch.from_numpy(win)  # (62, win_len, 5)
                    feats = []
                    for f in range(5):
                        tmp = temporal_learner(wt[:, :, f].unsqueeze(-1))  # (62,150)
                        red = fusion_fc(tmp)                               # (62,1)
                        feats.append(red)
                    feat62x5 = torch.cat(feats, dim=-1).cpu().numpy()     # (62,5)

                if i in test_trials:
                    test_samples.append(feat62x5)
                    test_labels.append(label_sessions[i-1])
                else:
                    train_samples.append(feat62x5)
                    train_labels.append(label_sessions[i-1])

        if not train_samples or not test_samples:
            raise ValueError(f"Empty split for {subject}, fold {fold_idx}")

        # 4) åˆå¹¶ã€æ ‡å‡†åŒ–ã€è¿”å›
        x_tr = np.stack(train_samples)  # (N_train, 62, 5)
        y_tr = np.array(train_labels, dtype=np.int64)
        x_ts = np.stack(test_samples)   # (N_test, 62, 5)
        y_ts = np.array(test_labels,  dtype=np.int64)

        # æŒ‰é€šé“æ ‡å‡†åŒ–
        x_tr, x_ts = standardize_data_per_channel(x_tr, x_ts)

        return x_tr, y_tr, x_ts, y_ts

    mat_path = os.path.join(path, subject)
    x_tr, y_tr, x_ts, y_ts = parse_mat_file(mat_path)

    return {
        "x_tr": x_tr,
        "y_tr": y_tr,
        "x_ts": x_ts,
        "y_ts": y_ts
    }


import numpy as np


def standardize_data_per_channel(train_data, test_data):
    """
    å¯¹ train_data å’Œ test_data æŒ‰ç…§é€šé“ç»´åº¦ï¼ˆaxis=1ï¼‰è¿›è¡Œæ ‡å‡†åŒ–ã€‚
    """
    # é¿å…å‡ºç°é™¤ 0
    eps = 1e-8

    # è®¡ç®—æ¯ä¸ªé€šé“çš„å‡å€¼å’Œæ ‡å‡†å·®
    mean_ = train_data.mean(axis=(0, 2), keepdims=True)
    std_ = train_data.std(axis=(0, 2), keepdims=True)

    # é˜²æ­¢ std_ ä¸º 0ï¼Œé¿å…é€šé“æ ‡å‡†å·®ä¸º0çš„æƒ…å†µ
    std_ = std_ if np.all(std_ > eps) else eps

    # å¯¹è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
    train_data = (train_data - mean_) / std_
    test_data = (test_data - mean_) / std_

    return train_data, test_data


def normalize(features, select_dim=0):
    # å¦‚æœ features æ˜¯ numpy æ•°ç»„ï¼Œå…ˆè½¬æ¢æˆ Tensor
    if isinstance(features, np.ndarray):
        features = torch.from_numpy(features)
    features_min, _ = torch.min(features, dim=select_dim)
    features_max, _ = torch.max(features, dim=select_dim)
    # ä¿è¯ç»´åº¦å¯¹é½ï¼ˆä¾‹å¦‚ï¼Œunsqueeze åœ¨ select_dim ä½ç½®ï¼‰
    features_norm = (features - features_min.unsqueeze(select_dim)) / (features_max - features_min).unsqueeze(
        select_dim)
    return features_norm


def standardize(features, select_dim=0):
    """
    å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆé›¶å‡å€¼ï¼Œå•ä½æ–¹å·®ï¼‰ã€‚
    å¦‚æœ features æ˜¯ numpy æ•°ç»„ï¼Œå…ˆè½¬æ¢æˆ Tensor
    select_dim: è¦æ ‡å‡†åŒ–çš„ç»´åº¦ (0 è¡¨ç¤ºæŒ‰é€šé“ï¼Œ1 è¡¨ç¤ºæŒ‰æ—¶é—´æ­¥ï¼Œ2 è¡¨ç¤ºæŒ‰ç‰¹å¾ç­‰)
    """
    if isinstance(features, np.ndarray):
        features = torch.from_numpy(features)

    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    features_mean = features.mean(dim=select_dim, keepdim=True)
    features_std = features.std(dim=select_dim, keepdim=True)

    # é˜²æ­¢æ ‡å‡†å·®ä¸º0çš„æƒ…å†µ
    eps = 1e-8
    features_std = features_std if torch.all(features_std > eps) else eps

    # æ ‡å‡†åŒ–
    features_standardized = (features - features_mean) / features_std
    return features_standardized

